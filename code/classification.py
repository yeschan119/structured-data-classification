{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sys import argv\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "train_path = sys.argv[1]\n",
    "test_path = sys.argv[2]\n",
    "result_path = sys.argv[3]\n",
    "train_file = open(train_path, \"r\")\n",
    "test_file = open(test_path, \"r\")\n",
    "\n",
    "train_data = pd.read_csv(train_file, sep='\\t')\n",
    "test_data = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "train_attributes = list(train_data.columns) # 전체 column얻기\n",
    "target_column = train_attributes.pop()  # class를 결정하는 target column 따로 분리\n",
    "class_list = np.unique(train_data[target_column]) #class를 나누기\n",
    "test_attributes = []  # infoGain으로 정해지는 atrributes을 리스트로 만들기\n",
    "\n",
    "# 해당 element가 데이터에서 차지하는 확률을 구하는 함수\n",
    "# elements인자는 리스트 형태로 들어오고, 리스트는 서로 다른 속성들의 개수를 포함(<=30[yes : 3, no : 2])\n",
    "# 각 속성들의 개수(elements[i])를 전체 개수(np.sum(elements))로 나누면 해당 element의 확률\n",
    "def get_prob(elements):\n",
    "    p = []\n",
    "    for i in range(len(elements)):\n",
    "        p.append(elements[i]/np.sum(elements))\n",
    "    return p\n",
    "\n",
    "# entropy, 즉 heterogeneous의 정도를 찾아 리턴하는 함수\n",
    "def get_entropy(an_attribute):\n",
    "    # 임이의 attibute을 넘겨 받아, 이 attribute이 포함하는 데이터들 중\n",
    "    # unique한 값들의 리스트와 그 개수를 리턴\n",
    "    unique_elements, counts = np.unique(an_attribute, return_counts = True)\n",
    "    # unique한 값들의 개수를 포함한 리스트를 이용해 확률 계산\n",
    "    p = get_prob(counts)\n",
    "    # 각각의 값들에 대한 확률은 리스트로 전달되어\n",
    "    # 확률 리스트를 인덱싱하면서 entropy 계산\n",
    "    entropy = 0\n",
    "    for i in range(len(unique_elements)):\n",
    "        entropy += -(p[i] * np.log2(p[i]))\n",
    "    return entropy\n",
    "\n",
    "# information_gain을 얻는 함수\n",
    "# 트리의 각 branch 데이터와 그 sub root, 타겟을 받아서\n",
    "# 전체 엔트로피와 자식노드들의 엔트로피를 구해서 info_gain 리턴\n",
    "def get_InfoGain(data,sub_root,target_name):\n",
    "    # 전체 엔트로피 계산\n",
    "    total_entropy = get_entropy(data[target_name])\n",
    "    \n",
    "    # 각 subTree 의 엔트로피 계산\n",
    "    #unique한 attribute의 리스트를 추출\n",
    "    sub_trees,counts= np.unique(data[sub_root],return_counts=True)\n",
    "    # 각각의 리스트에 대한 확률 계산\n",
    "    p = get_prob(counts)\n",
    "    # 각각의 attribute에 대한 확률에 그 attribute의 엔트로피를 곱한 값들의 합이\n",
    "    # sub_root의 전체 info-gain이 된다.\n",
    "    # dataframe.where를 이용해 전체 sub tree에서 각 branch에 해당하는 값들만 추출\n",
    "    # where 조건에 해당하지 않는 나머지 값들(Nan)은 dropna()를 이용해 제거\n",
    "    # 남은 데이터 중 target_column이 가지고 있는 데이터만 추출하여 entropy함수에 인자로 전달\n",
    "    sub_entropy = 0\n",
    "    for i in range(len(sub_trees)):\n",
    "        sub_entropy += p[i] * get_entropy(\n",
    "            data.where(data[sub_root] == sub_trees[i]).dropna()[target_name])\n",
    "    \n",
    "    Info_Gain = total_entropy - sub_entropy\n",
    "    return Info_Gain\n",
    "\n",
    "# 트리를 만드는 함수\n",
    "# data : 전체 데이터에서 recursive과정에 subtree로 분리되는 데이터\n",
    "# originaldata : 변하지 않는 전체 데이터\n",
    "# target_attribute : 모델을 정하는 기준 값(column), 트리의 leaf에 위치\n",
    "# train_attributes : target_attribute을 제외한 나머지 column들\n",
    "# parent_node : 첫 파라미터는 root이므로 parent_node를 None으로 설정\n",
    "def BuildTree(data,originaldata,train_attributes,target_attribute,parent_node = None):\n",
    "    \n",
    "    # no samples left의 경우 parent 반환\n",
    "    if len(train_attributes) == 0:\n",
    "        return parent_node\n",
    "    # all samples are in the same class\n",
    "    # 해당 데이터의 target attribute 값을 반환\n",
    "    elif len(np.unique(data[target_attribute])) <= 1:\n",
    "        return np.unique(data[target_attribute])[0]\n",
    "\n",
    "    # no remaining attributes\n",
    "    # 이 경우 original data에서 최대다수를 이루는 값 반환\n",
    "    elif len(data)==0:\n",
    "        # original_data에서 unique한 값들의 개수리스트를 추출\n",
    "        unique_counts = np.unique(originaldata[target_attribute], return_counts=True)[1]\n",
    "        # 그 중 가장 큰 값의 인덱스를 추출\n",
    "        max_index = np.argmax(unique_counts)\n",
    "        # original-data를 다시 인덱싱하여 최대다수의 값을 리턴\n",
    "        return np.unique(originaldata[target_attribute])[max_index]\n",
    "\n",
    "    # 트리 키우기\n",
    "    else:\n",
    "        # define the attribute of parent node\n",
    "        # 부모노드는 전체 데이터에서 최대다수에 포함되는 속성으로 정의(yes or no or others)\n",
    "        max_index = np.argmax(np.unique(originaldata[target_attribute], return_counts=True)[1])\n",
    "        parent_node = np.unique(originaldata[target_attribute])[max_index]\n",
    "        \n",
    "        # 각각의 속성들을 선택했을 때의 information gain을 얻어 리스트로 저장\n",
    "        infoGain_list = []\n",
    "        for attribute in train_attributes:\n",
    "            infoGain_list.append(get_InfoGain(data, attribute, target_attribute))\n",
    "        # 이중 가장 큰 값(infoGain)의 인덱스를 추출하여, 그 인덱스에 해당하는  attribute을 root로 지정\n",
    "        best_attr_index = np.argmax(infoGain_list)\n",
    "        root = train_attributes[best_attr_index]\n",
    "        test_attributes.append(root)\n",
    "        # root을 이용해 json구조로 트리 생성\n",
    "        tree = {root:{}}\n",
    "        \n",
    "        # root을 하나씩 제거하면서 recursive 실행 시 데이터를 분할(divide & conquer)\n",
    "        for i in train_attributes:\n",
    "            if i == root:\n",
    "                train_attributes.remove(i)\n",
    "        \n",
    "        # recursive를 실행하면서 branch 키우기\n",
    "        for branch in np.unique(data[root]):\n",
    "            # root가 포함한 값들을 unique을 기준으로 클러스터링\n",
    "            # df.where 함수를 이용하여 각 branch value에 맞는 값만 sub_data에 저장\n",
    "            sub_data = data.where(data[root] == branch).dropna()\n",
    "            \n",
    "            # recursive action\n",
    "            # 추출한 sub_data들로 다시 함수를 반복적으로 호출하면서 트리 키우기\n",
    "            subtree = BuildTree(sub_data,data,train_attributes,target_attribute,parent_node)\n",
    "            tree[root][branch] = subtree\n",
    "            \n",
    "        return(tree)\n",
    "    \n",
    "#만든 tree와 test data를 이용하여 결과를 예측해보기\n",
    "def Predict(tree, dic):\n",
    "    if tree in class_list:\n",
    "        return tree\n",
    "    for key in dic.keys():\n",
    "        if key in tree.keys():\n",
    "            subtree = tree[key]\n",
    "            break\n",
    "        elif dic[key] in tree.keys():\n",
    "            subtree = tree[dic[key]]\n",
    "            break\n",
    "        \n",
    "    return Predict(subtree, dic)\n",
    "\n",
    "# test 해보기\n",
    "#infoGain으로 얻은 column들의 순서를 target_list로 정하고\n",
    "#각 row마다 target_list에 대응하는 값들을 column : value로 저장하고\n",
    "#다 저장된 row를 한 줄씩 predict 함수로 보내 result를 예측\n",
    "#실행이 끝난 후에 result리스트를 test데이터에 하나의 column으로 concatenate 하기\n",
    "def Test_tree(tree):\n",
    "    dic = {}\n",
    "    result = []\n",
    "    for i in range(len(test_data)):\n",
    "        for j in test_attributes:\n",
    "            dic[j] = test_data[j][i]\n",
    "        result.append(Predict(tree,dic))\n",
    "    return result\n",
    "# main 함수\n",
    "if __name__ == '__main__':\n",
    "    # tree 만들기\n",
    "    tree = BuildTree(train_data, train_data, train_attributes,target_column)\n",
    "    #pprint(tree)\n",
    "    \n",
    "    test_data[target_column] = Test_tree(tree)\n",
    "    result_file = open(result_path, \"w\")\n",
    "    result_file.write(test_data.to_string())\n",
    "    train_file.close()\n",
    "    test_file.close()\n",
    "    result_file.close()\n",
    "    \n",
    "    print(\"Result output completed!!\")\n",
    "    print(\"execution time :\", str(round((time.time() - start),1)) +'s')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
